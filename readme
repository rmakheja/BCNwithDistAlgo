
PLATFORM.  

We are using python version 3.5.2 with DistAlgo version 1.0.9.
We are using python-library PyNaCl for computing digital signatures, computing hashes and sending encrypted messages
between different node processes. We have used VMs on Google Cloud Platform for testing on multiple hosts.
Operating system used on local laptops were Darwin Kernel Version 16.4.0.
Operating system used on Google Clound platform VMs were Linux instance-4 4.9.0-3-amd64 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux

INSTRUCTIONS.  
First, we start all the slave nodes with -D flag.
Once all slave nodes are up and running, we start the masterNode that creates and assigns all the processes to those slave nodes.

e.g. Start OlympusNode, ReplicaNode and ClientNode as slave nodes as follows
python3 -m da -H 127.0.0.1 --message-buffer-size=$((64*1024)) -f --logfilename case0.log -n OlympusNode -D Olympus.da
python3 -m da -H 127.0.0.1 --message-buffer-size=$((64*1024)) -f --logfilename case0.log -n ReplicaNode -D Replica.da
python3 -m da -H 127.0.0.1 -f --logfilename case0.log --message-buffer-size=$((64*1024)) -n ClientNode -D Client.da

Once all slaves are start, start the master as follows -
python3 -m da -H 127.0.0.1 --message-buffer-size=$((64*1024)) -n MainNode  --master __init__.da case3_system.conf

We can test the same working on multiple hosts by providing the host IPs while initializing these processes and in __init__.da while creating these process.
We have tested the multiple hosts working on following GCP VMs -
python3 -m da -H 10.142.0.2 --message-buffer-size=$((64*1024)) -f --logfilename case0.log -n OlympusNode -D Olympus.da
python3 -m da -H 10.142.0.3 --message-buffer-size=$((64*1024)) -f --logfilename case0.log -n ReplicaNode -D Replica.da
python3 -m da -H 10.142.0.4 -f --logfilename case0.log --message-buffer-size=$((64*1024)) -n ClientNode -D Client.da

python3 -m da -H 10.142.0.5 --message-buffer-size=$((64*1024)) -n MainNode  --master __init__.da <CONFIG_FILE_NAME>


WORKLOAD GENERATION:
The pseudorandom workload genrator generates a mixed workload. It supports both integer and string values. Every parameter is generated randomly, thus giving a good diversity. It can generate both success cases (where 'put' for some 'key' happens before any other operation on that 'key') and also failure cases (where 'get'/'slice'/'append' can happen before a 'put' on some 'key' or 'slice' operation can have invalid 'start'/'end' indices). the random string values generated are then surrounded by '' to replicate the input from config file. The algorithm used is as follows:

1)using seed, generate random numbers
2)Generate n operations as :
	i)	Get a random integer and use 'modulus 4' to get the type of operation to be performed
	ii) Randomly decide the type of keys and values i.e. integer or string(for string, the length of key/value is limited to 5)
	iii)For put and append operations, generate two random values based on their type.
			if the type == 0 : random.randint(0,10) ##to get int value
			if the type == 1 : ## get a random string of length 5
	            key ="\'""
	            for i in range(5): 
	                key += random.choice(string.ascii_letters + string.digits) 
	            key += "'"
		For get and slice operations, generate a key based on the type

		For slice, get two more random integers for start and end index.
	iv) add these n operations to a list 


BUGS AND LIMITATIONS.  a list of all known bugs in and limitations of your code.

1. Sometimes the system goes in infinite loop for get_running_state calling running state again and again due to wrong random quorum selection .
So need to restart the code so that right quorum selection could be done. This happens rarely though.

2. Client doesn't send reconig request because be have designed client to be a passive stackholder in the desgin. Replicas makes sure in detecting the problem or misbehaviour and triggering a reconfig.

CONTRIBUTIONS.  

Sanket Dige

1) Logging	
2) Multi-host execution
3) Encryption and decryption of messages
4) Hashing of result
5) (Replica)Dictionary object: support put, get, slice, append
6) Failure free flow of messages 
7) Workload extraction from config file
8) Reconfiguration with wedge, wedged, longest history then create and seed new history state on replicas
9) Olympus sends the information to client whenever change in configuration is success.
10) Catchup - caughtup request handling with get_running_state

Rashmi Makheja

1) Retransmission flow(Client and Replica)
2) Pseudorandom workload generation
3) Failure injection (triggers and failures)
4) Timeout
5) (Client) Generate request sequence from workload
6) (Client) Validating test case :check that dictionary contains expected content at end of test case
7) Documentation and test case generation
8) Chekcpointing initiate periodically and start checkpoint shuttle
9) Validate checkpoint shuttle and delete history prefix




MAIN FILES.  

__init__.da ==> main file that starts the client and olympus. It also parses the config file, generate the pseudorandom workloads.
Olympus.da ==> olympus code, also starts replicas
Replica.da ==> Replica code
Client.da ==> Client code


CODE SIZE.  

__init__.da ==>  Algorithm - 140 Other - 33 Total - 173
Olympus.da 	==>	 Algorithm - 257 Other - 264 Total - 521
Replica.da  ==>  Algorithm - 476 Other - 279 Total - 755
Client.da 	==>  Algorithm - 127 Other 133 - Total - 260

Used CLOC https://github.com/AlDanial/cloc) to get the lines of code. Manually calculated the 'other' lines

Roughly 70% of the "algorithm" code is for the algorithm itself

LANGUAGE FEATURE USAGE. 

list comprehensions  			==>  12
dictionary comprehensions		==>  13
set comprehensions				==> 4
Aggregations					==> None
Quantifications					==> None

OTHER COMMENTS

Whenever we initiate the reconfiguration of the replicas, a stable state from consistent quorum of replicas is computed and new set of replicas are created. This new set of replicas are seeded with the newly computed stable state and old replicas are aborted. We reset all the paramters in replicas
keeping eveything clean expect it state changes to stable state.

PERFORMANCE COMPARISION BETWEEN RAFT vs BCR

Test case used for perfomace comparision - 'perform900'

BCR time - elapsed time (seconds):  18.822170972824097
RAFT time - elapsed time (seconds):  8.799721002578735
